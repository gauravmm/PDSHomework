{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Text Classification\n", "\n", "In this problem, you will be analyzing the Twitter data we extracted using [this](https://dev.twitter.com/overview/api) api. This time, we extracted the tweets posted by the following six Twitter accounts: `realDonaldTrump, mike_pence, GOP, HillaryClinton, timkaine, TheDemocrats`.\n", "\n", "For every tweet, we collected two pieces of information:\n", "- `screen_name`: the Twitter handle of the user tweeting and\n", "- `text`: the content of the tweet.\n", "\n", "We divided the tweets into two parts - the train and test sets.  The training set contains both the `screen_name` and `text` of each tweet; the test set only contains the `text`.\n", "\n", "The overarching goal of the problem is to infer the political inclination (whether **R**epublican or **D**emocratic) of the author from the tweet text. The ground truth (i.e., true class labels) are determined from the `screen_name` of the tweet as follows:\n", "- **R**: `realDonaldTrump, mike_pence, GOP`\n", "- **D**: `HillaryClinton, timkaine, TheDemocrats`\n", "\n", "We can treat this as a binary classification problem. We'll follow this common structure to tackling this problem:\n", "\n", "1. **preprocessing**: clean up the raw tweet text using the various functions offered by [the Natural Language Toolkit (`nltk`)](http://www.nltk.org/genindex.html).\n", "2. **features**: construct bag-of-words feature vectors.\n", "3. **classification**: learn a binary classification model using [`scikit-learn`](http://scikit-learn.org/stable/modules/classes.html). \n", "\n", "Note that `nltk` supports optional corpora, toy grammars, trained models, etc. For this assignment, you have to manually install the stopwords list and `WordNetLemmatizer`. We'll begin by installing them:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import nltk\n", "import collections\n", "import string\n", "import numpy as np\n", "import sklearn\n", "import gzip\n", "import csv\n", "import re\n", "import matplotlib.pyplot as plt\n", "\n", "from testing.testing import test\n", "\n", "def nltk_download_test(nltk_download):\n", "    nltk_download()\n", "    try:\n", "        lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()\n", "        test.true(lemmatizer is not None)\n", "        stopwords=nltk.corpus.stopwords.words('english')\n", "        test.true(stopwords is not None)\n", "    except LookupError:\n", "        test.true(False)\n", "        \n", "@test\n", "def nltk_download():\n", "    nltk.download('stopwords')\n", "    nltk.download('wordnet')\n", "    nltk.download('punkt')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. Text Processing\n", "\n", "You first task to fill in the following function which processes and tokenizes raw text. The tokens must:\n", "\n", "1. be in lower case.\n", "2. appear in the same order as in the raw text.\n", "3. be in their lemmatized form, if one exists. If a word cannot be lemmatized, do not include it in the output.\n", "4. **not** contain any characters other than numbers and digits; you should:\n", "   1. remove trailing `'s`: `Children's` becomes `children`\n", "   2. omit other apostrophes: `don't` becomes `dont`\n", "   3. break tokens at other punctuation and/or unicode characters: `word-of-mouth` becomes `word`, `of`, `mouth` \n", "5. if the lemmatized form is a stopword, it should not appear in the output\n", "6. not include the parts of any t.co urls. Many tweets contain URLs from the domain `t.co`; you should strip all such URLs.\n", "\n", "If you figure out the right order to perform these operations, solving this problem is much easier.\n", "\n", "**Stopwords** are words that appear very often in text, usually playing a grammatical role (\"and\", \"a\", etc.). When comparing text similarity, these are not very useful; so we eliminate them at this stage. (NLTK provides us with a list of stopwords for English, which we will use later.)\n", "\n", "Hints:\n", "\n", " - you should use `nltk.word_tokenize()` in your solution\n", " - you should break tokens at all characters that are not in `string.ascii_letters` or `string.digits`\n", " - test your URL stripping! It's very easy to make a mistake with it.\n", " - When lemmatizing, you should convert the output to `str` (i.e. `lemmatized_word = str(lemmatizer.lemmatize(word))`) because the lemmatizer may return a non-string object."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def preprocess_test(preprocess):\n", "    test.equal(preprocess(\"I'm doing well! How about you?\"), ['im', 'doing', 'well', 'how', 'about', 'you'])\n", "    test.equal(preprocess(\"Education is the ability to listen to almost anything without losing your temper or your self-confidence.\"),    ['education', 'is', 'the', 'ability', 'to', 'listen', 'to', 'almost', 'anything', 'without', 'losing', 'your', 'temper', 'or', 'your', 'self', 'confidence'])\n", "\n", "    # Punctuation and space handling\n", "    test.equal(preprocess(\" a..a. .a . a.\"), ['a', 'a', 'a', 'a'])\n", "    test.equal(preprocess(\"word-of-mouth self-esteem\"), ['word', 'of', 'mouth', 'self', 'esteem'])\n", "\n", "    # Apostrophe handling\n", "    test.equal(preprocess(\"you've\"), ['youve'])\n", "    test.equal(preprocess(\"She's\"), ['she'])\n", "    test.equal(preprocess(\"SHE'S\"), ['she'])\n", "    test.equal(preprocess(\"Cea'sar\"), ['ceaar']) # You can assume that there are no mid-word \"'s\" substrings.\n", "\n", "    # Lemmatizer\n", "    test.equal(preprocess(\"walks\"), ['walk'])\n", "    \n", "    # Stopwords\n", "    stopwords = set(nltk.corpus.stopwords.words('english'))\n", "    test.equal(preprocess(\"I'm doing well! How about you?\", stopwords), ['im', 'well'])\n", "    test.equal(preprocess(\"Education is the ability to listen to almost anything without losing your temper or your self-confidence.\", stopwords), ['education', 'ability', 'listen', 'almost', 'anything', 'without', 'losing', 'temper', 'self', 'confidence'])\n", "\n", "    # Unicode handling\n", "    test.equal(preprocess(\"doot\ud83d\udc4fdoot\"), [\"doot\", \"doot\"])\n", "\n", "    # URL handling\n", "    test.equal(preprocess(\"http://t.co/WJs5bmRthU,https://t.co/WJs5bmRthU,\"), [])\n", "    test.equal(preprocess(\"boohttp://t.co/WJs5bmRthUhello\"), [\"boo\", \"hello\"])\n", "    test.equal(preprocess(\"https://t.co/ZhEy\u00c4\u00b6aaaa\"), ['http', 't', 'co', 'zhey', 'aaaa'])\n", "\n", "    # From the training set:\n", "    SW=set(nltk.corpus.stopwords.words('english')) | set([\"http\", \"co\", \"rt\", \"amp\"])\n", "    test.equal(preprocess('RT @GOPconvention: #Oregon votes today. That means 62 days until the @GOPconvention! https://t.co/OoH9FVb7QS', stopwords=SW), ['gopconvention', 'oregon', 'vote', 'today', 'mean', '62', 'day', 'gopconvention'])\n", "    test.equal(preprocess('RT @DWStweets: The choice for 2016 is clear: We need another Democrat in the White House. #DemDebate #WeAreDemocrats http://t.co/0n5g0YN46f', stopwords=SW), ['dwstweets', 'choice', '2016', 'clear', 'need', 'another', 'democrat', 'white', 'house', 'demdebate', 'wearedemocrats'])\n", "    test.equal(preprocess(\"Trump's calling for trillion dollar tax cuts for Wall Street.\\n\\nIt's time for them to pay their fair share. https://t.co/y8vyESIOES\", stopwords=SW), ['trump', 'calling', 'trillion', 'dollar', 'tax', 'cut', 'wall', 'street', 'time', 'pay', 'fair', 'share'])\n", "    test.equal(preprocess(\".@TimKaine's guiding principle: the belief that you can make a difference through public service. https://t.co/YopSUeMqOX\", stopwords=SW), ['timkaine', 'guiding', 'principle', 'belief', 'make', 'difference', 'public', 'service'])\n", "    test.equal(preprocess('Glad the Senate could pass a #THUD / MilCon / VetAffairs approps bill with solid provisions for Virginia: https://t.co/NxIgRC3hDi', stopwords=SW), ['glad', 'senate', 'could', 'pas', 'thud', 'milcon', 'vetaffairs', 'approps', 'bill', 'solid', 'provision', 'virginia'])\n", "    test.equal(preprocess('RT @IndyThisWeek: An @rtv6 exclusive: @GovPenceIN sits down with @RafaelOnTV\\nSee it Sunday morning at 8:30a on RTV6 and our RTV6 app. http:\u2026', stopwords=SW), ['indythisweek', 'rtv6', 'exclusive', 'govpencein', 'sits', 'rafaelontv', 'see', 'sunday', 'morning', '8', '30a', 'rtv6', 'rtv6', 'app'])\n", "    test.equal(preprocess('From Chatham Town Council to Congress, @RepRobertHurt has made a strong mark on his community. Proud of our work together on behalf of VA!', stopwords=SW), ['chatham', 'town', 'council', 'congress', 'reproberthurt', 'ha', 'made', 'strong', 'mark', 'community', 'proud', 'work', 'together', 'behalf', 'va'])\n", "    test.equal(preprocess('Thank you New Orleans, Louisiana!\\n#MakeAmericaGreatAgain #VoteTrump\\nhttps://t.co/tI1h9xT9GX https://t.co/0bf7BOlWEj', stopwords=SW), ['thank', 'new', 'orleans', 'louisiana', 'makeamericagreatagain', 'votetrump'])\n", "\n", "@test\n", "def preprocess(text, stopwords={}, lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()):\n", "    \"\"\" Normalizes case and handles punctuation\n", "    \n", "    args:\n", "        text: str -- raw text\n", "        stopwords : Set[str] -- lemmatized tokens to exclude from the output\n", "        lemmatizer : Lemmatizer -- an instance of a class implementing the lemmatize() method\n", "\n", "    Outputs:\n", "        list(str): tokenized text\n", "    \"\"\"\n", "    pass"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We give you some code that uses `preprocess` to prepare the data. This should take no more than 6s to run; if it takes longer than that, you need to make your preprocessing function run quicker."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# %%timeit\n", "## Uncomment the previous line to time your code. Remember to comment it out before uploading your solution.\n", "\n", "def read_data_test(read_data):\n", "    data_train, data_test = read_data()\n", "    \n", "    test.equal(len(data_train), 17298)\n", "    test.equal(len(data_test), 1000)\n", "    \n", "    #print(data_train[:8])\n", "\n", "def read_csv(stem, process=lambda x: x):\n", "    with gzip.open(f\"{stem}.csv.gz\", \"rt\", newline='', encoding=\"UTF-8\") as file:\n", "        csvr = csv.reader(file)\n", "        next(csvr)\n", "        return list(map(process, csvr))\n", "\n", "def is_republican(r):\n", "    return r in [\"realDonaldTrump\", \"mike_pence\", \"GOP\"]\n", "\n", "@test\n", "def read_data(extra_stopwords=set()):\n", "    \"\"\"Reads the dataset from the csv.gz files\n", "    \n", "    return : Tuple[data_train, data_test]\n", "        data_train : List[Tuple[is_republican, tokenized_tweet]]\n", "            is_republican : bool -- True if tweet is from a republican\n", "            tokenized_tweet : List[str] -- the tweet, tokenized by preprocess()\n", "    \"\"\"\n", "    stopwords = set(nltk.corpus.stopwords.words('english')) | set([\"http\", \"co\", \"rt\", \"amp\"]) | extra_stopwords\n", "    data_train = read_csv(\"tweets_train\", process=lambda r: (is_republican(r[0]), preprocess(r[1], stopwords)))\n", "    data_test = read_csv(\"tweets_test\", process=lambda r: preprocess(r[0], stopwords))\n", "    \n", "    return (data_train, data_test)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Feature Construction\n", "\n", "The next step is to derive feature vectors from the tokenized tweets. In this section, you will be constructing a bag-of-words [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) feature vector.\n", "\n", "The number of possible words is prohibitively large, and not all words are useful for our task. We will begin by filtering the vectors using a common heuristic:\n", "\n", "We calculate a frequency distribution of words in the corpus, and remove words at the head (most frequent) and tail (least frequent) of the distribution. Most frequently used words (often called stopwords) provide very little information about the similarity of two pieces of text; we have already removed these. Words with extremely low frequency tend to be typos.\n", "\n", "We will now implement a function which counts the number of times that each token is used in the training corpus. You should return a [`collections.Counter`](https://docs.python.org/3/library/collections.html#collections.Counter) object with the number of times that each word appears in the dataset.\n", "\n", "(This should take no more than 20s to run, including reading the files.)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def get_distribution_test(get_distribution):\n", "    data_train, data_test = read_data()\n", "    dist = get_distribution(data_train)\n", "    test.true(isinstance(dist, collections.Counter))\n", "    if dist is None:\n", "        return\n", "\n", "    # Simple test cases:\n", "    test.equal(dist['trump'], 1812)\n", "    test.equal(dist['clinton'], 1107)\n", "    test.equal(dist['president'], 788)\n", "    test.equal(dist['american'], 745)\n", "    test.equal(dist['job'], 676)\n", "    test.equal(dist['obama'], 438)\n", "    test.equal(dist['hoosier'], 393)\n", "\n", "    # Check that you have the correct number of unique words:\n", "    test.equal(len(dist), 16762)\n", "    # There are 8048 words used exactly once, 2399 words used twice, etc:\n", "    test.equal(collections.Counter(dist.values()).most_common(5), [(1, 8048), (2, 2399), (3, 1198), (4, 778), (5, 577)])\n", "    \n", "    plt.hist(dist.values(), bins=100)\n", "    plt.yscale('log')\n", "\n", "@test\n", "def get_distribution(data_train):\n", "    \"\"\" Calculates the word count distribution, excluding stopwords.\n", "\n", "    args: \n", "        data_train -- the training data\n", "\n", "    return : collections.Counter -- the distribution of word counts\n", "    \"\"\"\n", "    pass"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Notice the distribution looks exponential, even with a logarithmic y-axis; there are a lot words that appear only once. Lets figure out what these words are so we can eliminate them from the dataset."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def get_rare_words_test(get_rare_words):\n", "    data_train, data_test = read_data()\n", "    dist = get_distribution(data_train)\n", "    new_stopwords = get_rare_words(dist)\n", "    \n", "    with open(\"get_rare_words.output\", \"rt\", newline=\"\") as f:\n", "        ref_stopwords = set(t.strip() for t in f)\n", "\n", "    test.true(isinstance(new_stopwords, set))\n", "    # Extra words in your solution:\n", "    test.equal(new_stopwords - ref_stopwords, set())\n", "    # Words missing from your solution:\n", "    test.equal(ref_stopwords - new_stopwords, set())\n", "\n", "@test\n", "def get_rare_words(dist):\n", "    \"\"\"use the word count information from the training data to find more stopwords\n", "\n", "    args:\n", "        dist: collections.Counter -- the output of get_distribution\n", "\n", "    returns : Set[str] -- a set of all words that appear exactly once in the training data\n", "    \"\"\"\n", "    pass"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Here we provide a wrapper function to cache the preprocessed data. This helps it not take quite as long to re-run. If you change anything above this cell, re-run this cell to clear the cache."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["global PREPROCESSED_DATA_CACHE\n", "PREPROCESSED_DATA_CACHE = None\n", "\n", "def get_data():\n", "    global PREPROCESSED_DATA_CACHE\n", "    if PREPROCESSED_DATA_CACHE is None:\n", "        data_train, data_test = read_data()\n", "        dist = get_distribution(data_train)\n", "        new_stopwords = get_rare_words(dist)\n", "        PREPROCESSED_DATA_CACHE = read_data(new_stopwords)\n", "\n", "    return PREPROCESSED_DATA_CACHE"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Vectorizing\n", "\n", "Now we have each tweet as a list of words, excluding words with high- and low-frequencies. We want to convert these into a sparse feature matrix, where each row corresponds to a tweet and each column to a possible word. We can use `scikit-learn`'s [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) to do this quite easily.\n", "\n", "There's a catch, though: `TfidfVectorizer` expects the input to be a string, and (by default) it perfoms its own analyzing. You have to override that behavior by passing in `do_nothing` to the constructor as an optional parameter.\n", "\n", "Hints:\n", "\n", " - Read [the documentation](https://scikit-learn.org/stable/modules/feature_extraction.html#customizing-the-vectorizer-classes) carefully, and then this [blog post](http://www.davidsbatista.net/blog/2018/02/28/TfidfVectorizer/) ([mirror](http://archive.is/pVdqE)). You need to pass in `do_nothing` in two locations.\n", " - You should use only the training data to `fit` or `fit_transform` the vectorizer."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Helper function, do not change:\n", "def do_nothing(x):\n", "    return x\n", "\n", "def create_features_test(create_features):\n", "    train_features, train_labels, test_features = create_features(*get_data())\n", "\n", "    test.equal(repr(train_features), \"\"\"<17298x8714 sparse matrix of type '<class 'numpy.float64'>'\n", "\twith 161480 stored elements in Compressed Sparse Row format>\"\"\")\n", "\n", "    test.equal(repr(test_features), \"\"\"<1000x8714 sparse matrix of type '<class 'numpy.float64'>'\n", "\twith 9037 stored elements in Compressed Sparse Row format>\"\"\")\n", "\n", "    test.equal(train_labels.dtype, bool)\n", "    test.equal(len(train_labels), 17298)\n", "    test.equal(sum(train_labels), 8646)\n", "\n", "@test\n", "def create_features(train_data, test_data):\n", "    \"\"\"creates the feature matrices and label vector for the training and test sets.\n", "\n", "    args:\n", "        train_data : List[Tuple[is_republican, tweet_words]]\n", "            is_republican : bool -- True if Republican, False otherwise\n", "            tweet_words : List[str] -- the processed tweet tokens\n", "        test_data : List[List[str]] -- a list of processed tweets\n", "\n", "    returns: Tuple[train_features, train_labels, test_features]\n", "        train_features : scipy.sparse.csr.csr_matrix -- feature matrix for the training set\n", "        train_labels : np.array[num_train] -- a numpy vector, where 1 stands for Republican and 0 stands for Democrat \n", "        test_features : scipy.sparse.csr.csr_matrix -- feature matrix for the test set\n", "    \"\"\"\n", "\n", "    train_labels = None\n", "    train_features = None\n", "    test_features = None\n", "\n", "    return (train_features, train_labels, test_features)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Observe that the created matrices are very sparse.\n", "\n", "Now that we have the features, lets perform the classification:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3. Classification\n", "\n", "We are ready to put it all together and train the classification model.\n", "\n", "You will be will be using the Support Vector Machine [`sklearn.svm.SVC`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC). [Here](http://docs.opencv.org/2.4/doc/tutorials/ml/introduction_to_svm/introduction_to_svm.html) is a quick introduction to SVMs.\n", "\n", "At the heart of an SVM is the concept of a _kernel function_, which determines the distance between two data points. `sklearn.svm.SVC` natively supports four kernel functions: `linear`, `poly`, `rbf`, `sigmoid`. For this problem space, we will use the `linear` kernel.\n", "\n", "In this section, we will:\n", "\n", "1. build a classifier using the `linear` kernel,\n", "2. train it using the training set,\n", "3. evaluate the trained model on the training set, and then\n", "4. use it to predict classification on our test set.\n", "\n", "Let's begin by training a classifier. This should take no more than 20s to run. You should set the optional parameter `gamma` to `auto`, but leave the rest at their default values."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def learn_classifier_test(learn_classifier):\n", "    train_features, train_labels, _ = create_features(*get_data())\n", "    classifier = learn_classifier(train_features, train_labels)\n", "\n", "    test.equal(repr(classifier).replace(\"\\n\", \"\").replace(\"  \", \" \").replace(\"  \", \" \"), \"\"\"SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False)\"\"\")\n", "\n", "@test\n", "def learn_classifier(train_features, train_labels, kernel=\"linear\"):\n", "    \"\"\"learns a classifier from the input features and labels using a specified kernel function\n", "\n", "    args:\n", "        train_features: scipy.sparse.csr.csr_matrix -- sparse matrix of features\n", "        train_labels : numpy.ndarray(bool): binary vector of class labels\n", "        kernel : str -- kernel function to be used with classifier, must be (linear|poly|rbf|sigmoid)\n", "\n", "    return : sklearn.svm.classes.SVC -- classifier\n", "    \"\"\"\n", "\n", "    assert kernel in [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]\n", "\n", "    pass # Implement this."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now that we know how to train a classifier, the next step is to measure its performance. This step is necessary to select the best model among a given set of models, or even tune hyperparameters for a given model.\n", "\n", "We would ordinarily use a held-out validation set to evaluate the performance of the classifier. The use of a held-out set prevents overfitting to the data, and you will do this for another assignment. For this problem, though, we can use the training set.\n", "\n", "To measure classification accuracy we will use the [$F_1$ score](https://en.wikipedia.org/wiki/F1_score). Implement this:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def f1_test(f1):\n", "    test.equal(f1([1,1,1], [1,1,1]), 1.0)\n", "    test.equal(f1([1,0,0], [1,0,0]), 1.0)\n", "    test.equal(f1([1,1,0], [1,1,1]), 0.8)\n", "    test.equal(f1([1,0,0], [1,1,0]), 2/3)\n", "    test.equal(f1([0,0,1], [1,0,1]), 2/3)\n", "    test.equal(f1([1,0,0], [1,1,1]), 0.5)\n", "\n", "@test\n", "def f1(pred, ground):\n", "    \"\"\" evaluates a classifier based on a supplied validation data\n", "\n", "    args:\n", "        pred: numpy.ndarray(bool) -- predictions\n", "        ground: numpy.ndarray(bool) -- known ground-truth values\n", "    \n", "    return : double -- the F1 score of the predictions\n", "    \"\"\"\n", "    pred = np.array(pred, dtype=bool)\n", "    ground = np.array(ground, dtype=bool)\n", "\n", "    pass # Implement this"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now we calculate the F1 score on the training set:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def evaluate_test(evaluate):\n", "    train_features, train_labels, _ = create_features(*get_data())\n", "    test.true(np.abs(evaluate(train_features, train_labels, 'linear') - 0.95389842) < 1e-5)\n", "\n", "@test\n", "def evaluate(train_features, train_labels, kernel=\"linear\"):\n", "    \"\"\"train the classifier and report the F1 score on the training set\n", "    \n", "    args:\n", "        train_features: scipy.sparse.csr.csr_matrix -- sparse matrix of features\n", "        train_labels : numpy.ndarray(bool): binary vector of class labels\n", "        kernel : str -- kernel function to be used with classifier, must be (linear|poly|rbf|sigmoid)\n", "\n", "    return : double -- the F1 score of the predictions on the training labels\n", "    \"\"\"\n", "    return 0.0"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Classifying Test Tweets\n", "\n", "Home stretch! Now we can classify the test tweets! Use `learn_classifier` to make a trained classifier and predict the labels given the `test_features`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def pp(entries):\n", "    from IPython.display import HTML, display\n", "    import tabulate\n", "\n", "    display(HTML(tabulate.tabulate([(f'<b>{\"R\" if isr else \"D\"}</b>', txt[0]) for isr, txt in entries], tablefmt='html')))\n", "\n", "def classify_tweets_test(classify_tweets):\n", "    test_original = read_csv(\"tweets_test\")\n", "    train_features, train_labels, test_features = create_features(*get_data())\n", "    test_classes = classify_tweets(train_features, train_labels, test_features)\n", "\n", "    pp([e for i, e in enumerate(zip(test_classes, test_original)) if i in [0, 2, 9, 70, 654, 723]])\n", "\n", "@test\n", "def classify_tweets(train_features, train_labels, test_features):\n", "    \"\"\"Train a model and predict class labels for the test set.\n", "\n", "    args:\n", "        train_features: scipy.sparse.csr.csr_matrix -- sparse matrix of features\n", "        train_labels : numpy.ndarray(bool): binary vector of class labels\n", "        test_features: scipy.sparse.csr.csr_matrix -- sparse matrix of features, test set\n", "\n", "    return : numpy.ndarray[bool] -- True if the corresponding tweet is predicted to be Republican, False otherwise.\n", "    \"\"\"\n", "    pass"]}], "metadata": {"anaconda-cloud": {}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.3"}}, "nbformat": 4, "nbformat_minor": 1}