{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Linear Regression\n", "\n", "In this homework we are going to apply linear regression to two different problems. We'll begin by guiding you through predicting job satisfaction and the desire to be a manager among developers based on survey data. Once that's done, you will model candy preference based on composition and food science properties"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import csv\n", "import gzip\n", "import math\n", "import hashlib\n", "import numpy as np\n", "from pprint import pprint\n", "\n", "from testing.testing import test"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Developers! Developers! Developers!\n", "\n", "The data from this question is based on the [2019 StackOverflow Survey](https://insights.stackoverflow.com/survey/2019); accordingly, the subset bundled with this assignment is also released under the Open Database License (ODbL) v1.0.\n", "\n", "The data was made by selecting some columns from the original dataset, only retaining rows from people who described themselves as \"a developer by profession\", and replaced long responses with shorter strings. Lets begin by examining the data."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def read_csv_test(read_csv):\n", "    headers, rows = read_csv()\n", "    test.equal(len(rows), 65679)\n", "    test.equal(len(headers), 26)\n", "\n", "    # Print a row:\n", "    pprint(dict(zip(headers, rows[0])))\n", "    \n", "@test\n", "def read_csv(fn=\"eggs.csv.gz\"):\n", "    \"\"\"read the GZipped CSV data and split it into headers and newlines.\n", "    \n", "    kwargs:\n", "        fn : str -- .csv.gz file to read\n", "    \n", "    returns: Tuple[headers, body] where\n", "      headers : Tuple[str] -- the CSV headers\n", "      body : List[Tuple[str,...]] -- the CSV body\n", "    \"\"\"\n", "    with gzip.open(fn, 'rt', newline=\"\", encoding='utf-8') as f:\n", "        csvobj = csv.reader(f)\n", "        headers = next(csvobj)\n", "        return headers, [tuple(row) for row in csvobj]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Our task is to predict:\n", "\n", "1. if respondents are managers or want to be a manager in the future (`MgrWant` is `y`), and\n", "2. if respondents are satisfied with their career (`CareerSat` is `vs` or `ss`)\n", "\n", "based on the remaining rows. We have bolded these rows in the table below.\n", "\n", "Before we can use linear regression, we must convert this into numeric data. This is the core challenge of this problem; Here's a table of rows and what they mean:\n", "\n", "\n", "| Column | Sample | Does/is the respondent... | Type/Values |\n", "| --- |:--- |:--- |:--- |\n", "| **CareerSat** | 'vs' | satisfied with their career? | (`vd`, `sd`, `ne`, `NA`, `ss`, `vs`) -- corresponding to ({very, slightly}, {satisfied, dissatisfied}), neutral, and not applicable |\n", "| **MgrWant** | 'n' | ...want to be a manager? | boolean |\n", "| Age    | '22'   | age | integer     |\n", "| CodeRevHrs | '2' | hours a week spent reviewing code | integer |\n", "| ConvertedComp | '61000' | yearly compensation in 2019 USD | integer |\n", "| Country | 'United States' | lives in country | string _(ignore in regression)_ |\n", "| Dependents | 'n' | ...have children or other dependents. | boolean |\n", "| DevEnvironVSC | 'y' | ...use Visual Studio Code | boolean |\n", "| DevTypeFullStack | 'n' | ...identify as a full-stack developer | boolean |\n", "| EdLevel | 'bachelors' | maximum education level | (`other`, `bachelors`, `masters`, `doctoral`) |\n", "| EduOtherMOOC | 'y' | ...ever taken a Massively Open Online Course | boolean |\n", "| EduOtherSelf | 'y' | ...ever taught themselves a new platform | boolean |\n", "| Extraversion | 'y' | ...prefer in-person meetings to online meetings | boolean |\n", "| GenderIsMan | 'y' | ...male | boolean |\n", "| Hobbyist | 'n' | ...write code as a hobby? | boolean |\n", "| MgrIdiot | 'very' | ...think their manager knows what they are doing? | (`NA`, `not`, `some`, `very`), in order of increasing confidence |\n", "| OpSys | 'win' | which OS do they use? | (`win`, `mac`, `tux`, `NA`), for (Windows, Mac OSX, Linux-like, NA) |\n", "| OpenSourcer | 'Never' | ...contribute to open-source projects? | (`never`, `year`, `month-year`, `month`), in increasing order of frequency |\n", "| OrgSize | '100-499' | number of employees in organization? | (`NA`, `1`, `2-9`, `10-19`, `20-99`, `100-499`, `500-999`, `1,000-4,999`, `5,000-9,999`, `10,000+`) |\n", "| Respondent | '4' | respondent ID from original data | integer _(ignore in regression)_ |\n", "| Student | 'n' | ...currently a student? | boolean |\n", "| UndergradMajorIsComputerScience | 'y' | ...majored in CS? | boolean |\n", "| UnitTestsProcess | 'n' | ...use unit tests in their job? | boolean |\n", "| WorkWeekHrs | '80' | hours a week worked | integer |\n", "| YearsCode | 3 | years since first programming | integer |\n", "| YearsCodePro | 0 | years programming professionally | integer |"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Type conversion\n", "\n", "Now for the slow data-cleaning grind that is characteristic of work as a data scientist. We begin by writing type coercion functions: functions that convert each column type into a `float` value for use in linear regression. All input values are `str`.\n", "\n", "The column types are:\n", "\n", " - _boolean_ : `y`/`NA`/`n` assigned to `+1.0`/`0.0`/`0.0`\n", " - _integer_ : convert to `float`, preserving value. `NA` equals `0.0`. \n", " - _string_ : not included in regression; we'll use it later\n", " - CareerSat: Map (`vd`, `sd`, `ne`, `NA`, `ss`, `vs`) to (-2.0, -1.0, 0.0, 0.0, 1.0, 2.0)\n", " - EdLevel: Map (`other`, `bachelors`, `masters`, `doctoral`) to (0.0, 1.0, 1.5, 2.0)\n", " - MgrIdiot: Map (`NA`, `not`, `some`, `very`) to (-1.0, -1.0, 0.0, 1.0)\n", " - OpSys: This is a category variable, we will split this into three columns (one for each possible value) and set 1.0 in the corresponding column. This is called a [one-hot encoding](https://en.wikipedia.org/wiki/One-hot). (Don't write a conversion function in this step.)\n", " - OpenSourcer : Map (`never`, `year`, `month-year`, `month`) to (0.0, 0.5, 1.0, 2.0)\n", " - OrgSize: Map each range \"$a$-$b$\" to the value $ln(a)$. Treat `NA` as `ln(1.0) = 0`. We are converting an exponentially distributed range to a linearly distributed one.\n", "\n", "All your conversion functions must throw an exception if you encounter an unexpected value. As an example, we give you the boolean conversion function:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def type_boolean_test(type_boolean):\n", "    test.true(isinstance(type_boolean(\"y\"), float))\n", "    test.equal(type_boolean(\"y\"), 1.0)\n", "    test.equal(type_boolean(\"n\"), 0.0)\n", "    test.exception(lambda: type_boolean(\"5\"))\n", "\n", "@test\n", "def type_boolean(c):\n", "    if c == \"y\": return 1.0\n", "    elif c == \"n\": return 0.0\n", "    elif c == \"NA\": return 0.0\n", "    raise ValueError(c)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now fill in these functions according to specification:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Integer\n", "def type_integer_test(type_integer):\n", "    test.true(isinstance(type_integer(\"5\"), float))\n", "    test.equal(type_integer(\"3\"), 3.0)\n", "    test.equal(type_integer(\"0\"), 0.0)\n", "    test.equal(type_integer(\"-4\"), -4.0)\n", "    test.equal(type_integer(\"NA\"), 0.0)\n", "    test.exception(lambda: type_integer(\"yes\"))\n", "\n", "@test\n", "def type_integer(c):\n", "    pass\n", "\n", "\n", "# CareerSat\n", "def type_CareerSat_test(type_CareerSat):\n", "    test.true(isinstance(type_CareerSat(\"vd\"), float))\n", "    test.equal(type_CareerSat(\"sd\"), -1.0)\n", "    test.equal(type_CareerSat(\"ne\"), 0.0)\n", "    test.equal(type_CareerSat(\"NA\"), 0.0)\n", "    test.equal(type_CareerSat(\"ss\"), 1.0)\n", "    test.equal(type_CareerSat(\"vs\"), 2.0)\n", "    test.exception(lambda: type_CareerSat(\"yes\"))\n", "\n", "@test\n", "def type_CareerSat(c):\n", "    pass\n", "\n", "\n", "# EdLevel\n", "def type_EdLevel_test(type_EdLevel):\n", "    test.true(isinstance(type_EdLevel(\"other\"), float))\n", "    test.equal(type_EdLevel(\"bachelors\"), 1.0)\n", "    test.equal(type_EdLevel(\"masters\"), 1.5)\n", "    test.equal(type_EdLevel(\"doctoral\"), 2.0)\n", "    test.exception(lambda: type_EdLevel(\"yes\"))\n", "\n", "@test\n", "def type_EdLevel(c):\n", "    pass\n", "\n", "\n", "# MgrIdiot\n", "def type_MgrIdiot_test(type_MgrIdiot):\n", "    test.true(isinstance(type_MgrIdiot(\"NA\"), float))\n", "    test.equal(type_MgrIdiot(\"not\"), -1.0)\n", "    test.equal(type_MgrIdiot(\"some\"), 0.0)\n", "    test.equal(type_MgrIdiot(\"very\"), 1.0)\n", "    test.exception(lambda: type_MgrIdiot(\"yes\"))\n", "\n", "@test\n", "def type_MgrIdiot(c):\n", "    pass\n", "\n", "\n", "# OpenSourcer\n", "def type_OpenSourcer_test(type_OpenSourcer):\n", "    test.true(isinstance(type_OpenSourcer(\"never\"), float))\n", "    test.equal(type_OpenSourcer(\"year\"), 0.5)\n", "    test.equal(type_OpenSourcer(\"month-year\"), 1.0)\n", "    test.equal(type_OpenSourcer(\"month\"), 2.0)\n", "    test.exception(lambda: type_OpenSourcer(\"yes\"))\n", "\n", "@test\n", "def type_OpenSourcer(c):\n", "    pass\n", "\n", "\n", "# OrgSize\n", "def type_OrgSize_test(type_OrgSize):\n", "    test.true(isinstance(type_OrgSize(\"1\"), float))\n", "    test.equal(type_OrgSize(\"NA\"), 0)\n", "    test.equal(type_OrgSize(\"2-9\"), 0.6931471805599453)\n", "    test.equal(type_OrgSize(\"100-499\"), 4.605170185988092)\n", "    test.equal(type_OrgSize(\"10,000+\"), 9.210340371976184)\n", "    test.exception(lambda: type_OrgSize(\"yes\"))\n", "\n", "@test\n", "def type_OrgSize(c):\n", "    pass"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now we deal with `OpSys`; from the one column in the source, create three columns (called `OpSysWin`, `OpSysMac`, and `OpSysTux`, corresponding to the values `win`, `mac`, `tux`.) For each row, at most one of the cells must be 1.0, and the others must be 0.0. If the value in the cell is `NA`, `BSD`, or something else, then all the cells must be 0.0.\n", "\n", "This is called a [one-hot encoding](https://en.wikipedia.org/wiki/One-hot) and is a common way to handle category variables.\n", "\n", "In `convert_data_stackoverflow`, you should:\n", "\n", "1. Encode `OpSys` as the one-hot encoding discussed above\n", "2. Remove the `Respondent` and `Country`, the two columns not used.\n", "3. Convert other columns using the appropriate functions above."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def convert_data_stackoverflow_test(convert_data_stackoverflow):\n", "    headers, rows = convert_data_stackoverflow(*read_csv())\n", "    # Correct number of rows:\n", "    test.equal(len(rows), 65679)\n", "\n", "    # If this test fails, your headers are incorrect:\n", "    test.equal(set(headers), {'CareerSat', 'MgrWant', 'Age', 'CodeRevHrs', 'ConvertedComp', 'Dependents', 'DevEnvironVSC', 'DevTypeFullStack', 'EdLevel', 'EduOtherMOOC', 'EduOtherSelf', 'Extraversion', 'GenderIsMan', 'Hobbyist', 'MgrIdiot', 'OpSysWin', 'OpSysMac', 'OpSysTux', 'OpenSourcer', 'OrgSize', 'Student', 'UndergradMajorIsComputerScience', 'UnitTestsProcess', 'WorkWeekHrs', 'YearsCode', 'YearsCodePro'})\n", "    # Type check:\n", "    test.true(all(all(isinstance(v, float) for v in r) for r in rows))\n", "    # Operating System columns:\n", "    for row in rows:\n", "        d = dict(zip(headers, row))\n", "        if sorted([d[\"OpSysWin\"], d[\"OpSysMac\"], d[\"OpSysTux\"]]) not in [[.0, .0, 1.], [0.]*3]:\n", "            test.true(False)\n", "            break\n", "    else:\n", "        test.true(\"There is correctly at most one OpSys* column set to 1.0\")\n", "    \n", "    # More direct tests\n", "    test.equal(dict(zip(headers, rows[-2])), {'CareerSat': -1.0, 'MgrWant': 1.0, 'Age': 0.0, 'CodeRevHrs': 5.0, 'ConvertedComp': 588012.0, 'Dependents': 1.0, 'DevEnvironVSC': 1.0, 'DevTypeFullStack': 1.0, 'EdLevel': 1.5, 'EduOtherMOOC': 0.0, 'EduOtherSelf': 0.0, 'Extraversion': 0.0, 'GenderIsMan': 1.0, 'Hobbyist': 1.0, 'MgrIdiot': -1.0, 'OpSysWin': 0.0, 'OpSysMac': 0.0, 'OpSysTux': 1.0, 'OpenSourcer': 0.0, 'OrgSize': 4.605170185988092, 'Student': 1.0, 'UndergradMajorIsComputerScience': 1.0, 'UnitTestsProcess': 1.0, 'WorkWeekHrs': 40.0, 'YearsCode': 10.0, 'YearsCodePro': 8.0})\n", "    test.equal(dict(zip(headers, rows[-1])), {'CareerSat': -1.0, 'MgrWant': 0.0, 'Age': 33.0, 'CodeRevHrs': 0.0, 'ConvertedComp': 22915.0, 'Dependents': 0.0, 'DevEnvironVSC': 0.0, 'DevTypeFullStack': 0.0, 'EdLevel': 1.0, 'EduOtherMOOC': 0.0, 'EduOtherSelf': 0.0, 'Extraversion': 1.0, 'GenderIsMan': 1.0, 'Hobbyist': 0.0, 'MgrIdiot': -1.0, 'OpSysWin': 0.0, 'OpSysMac': 0.0, 'OpSysTux': 1.0, 'OpenSourcer': 2.0, 'OrgSize': 2.995732273553991, 'Student': 0.0, 'UndergradMajorIsComputerScience': 1.0, 'UnitTestsProcess': 1.0, 'WorkWeekHrs': 48.0, 'YearsCode': 9.0, 'YearsCodePro': 5.0})\n", "\n", "@test\n", "def convert_data_stackoverflow(headers, data):\n", "    \"\"\"convert the data into \n", "    \n", "    args:\n", "        header : List[str] -- the header for each column in the CSV\n", "        data : List[Tuple[str]] -- the CSV data, where each inner list corresponds to a row in the CSV file.\n", " \n", "    returns: Tuple[headers, body] where\n", "      headers : List[str] -- the new headers, dropping the Country and Respondent headers and expanding \n", "      body : List[List[str,...]] -- the CSV body\n", "    \"\"\"\n", "    pass"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Splitting Data\n", "\n", "Now we prepare the converted data for regression. In this step, we:\n", "\n", " 1. split this into training and validation sets,\n", " 2. convert it to a Numpy `ndarray` with underlying type `np.float32`,\n", " 3. split each set into the predicted columns and the feature columns.\n", "\n", "We will save the first 20% of the dataset (rounded down) as the validation set and keep the remaining as the training set. (Note that it is common practice to randomize the dataset; this has already been done. Don't shuffle the dataset for this assignment.)\n", "\n", "Ensure that the underlying type of the `ndarray` is `np.float32`, not the default `np.float64`. We do not need the added precision of 64-bit floating point numbers for this problem, and using the smaller numbers will speed up computation and reduce the amount of memory we need."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def split_data_test(split_data):\n", "    headers, rows = convert_data_stackoverflow(*read_csv())\n", "    l = len(rows)\n", "    \n", "    val, train = split_data(rows)\n", "    test.equal(len(val), l // 5)\n", "    test.true(isinstance(val, np.ndarray))\n", "    test.equal(val.dtype, np.float32)\n", "    test.equal(len(train), l - (l // 5))\n", "    test.true(isinstance(train, np.ndarray))\n", "    test.equal(train.dtype, np.float32)\n", "\n", "@test\n", "def split_data(data):\n", "    \"\"\"split the data into training and validation sets, and convert them to np.ndarray. (Step 1 and 2 above.)\n", "\n", "    args:\n", "        data : List[List[str]] -- the CSV data, where each inner list corresponds to a row in the CSV file.\n", "\n", "    returns: Tuple[val, train] where\n", "      val  : np.ndarray[num_val_rows, num_features] -- the first 20% of the dataset (rounded down)\n", "      train : np.ndarray[num_train_rows, num_features] -- the remaining rows from data\n", "    \n", "    Ensure that the underlying type of the output is np.float32, not the default np.float64.\n", "    \"\"\"\n", "    pass"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def separate_objective_test(separate_objective):\n", "    headers, rows = convert_data_stackoverflow(*read_csv())\n", "    val, train = split_data(rows)\n", "\n", "    for subset in [val, train]:\n", "        subset_headers, subset_features, subset_objectives = separate_objective(headers, subset, [\"CareerSat\", \"MgrWant\"])\n", "\n", "        test.true(isinstance(subset_objectives, tuple))\n", "        test.equal(len(subset_objectives), 2)\n", "        test.true(\"CareerSat\" not in subset_headers)\n", "        test.true(\"MgrWant\" not in subset_headers)\n", "        test.equal(subset_features.shape[1], 24)\n", "\n", "@test\n", "def separate_objective(headers, data, objectives):\n", "    \"\"\"split the objective columns from the headers and data. (Step 1 and 2 above.)\n", "\n", "    args:\n", "        headers    : List[str] -- the headers for the data, used to find the objective columns from the data array\n", "        data       : np.ndarray[num_rows, num_columns] -- the data\n", "        objectives : the columns to extract from the data\n", "\n", "    returns: Tuple[o_headers, o_features, o_objectives] where\n", "      o_headers  : List[str] -- a list of headers without the objective columns\n", "      o_features : np.ndarray[num_train_rows, num_features] -- the remaining columns from data. (num_features = num_columns - len(objectives))\n", "      o_objectives : Tuple[np.ndarray[num_train_rows], ...] -- a list of objective columns from the data, each element is a 1-dimensional np.ndarray corresponding to the entry in objectives.\n", "     \"\"\"\n", "    pass"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Linear Regression\n", "\n", "Now you will finally implement a linear regression. As a reminder, linear regression models the data as\n", "\n", "$$\\mathbf y = \\mathbf X\\mathbf \\beta + \\mathbf \\epsilon$$\n", "\n", "where $\\mathbf y$ is a vector of outputs, $\\mathbf X$ is also known as the design matrix, $\\mathbf \\beta$ is a vector of parameters, and $\\mathbf \\epsilon$ is noise. We will be estimating $\\mathbf \\beta$ using the [Ordinary Least Squares](https://en.wikipedia.org/wiki/Ordinary_least_squares) approach.\n", "\n", "Hints:\n", "\n", " 1. Use `np.linalg.solve` to calculate `beta` instead of inverting the matrix, which is [numerically unstable](https://math.stackexchange.com/a/1622654).\n", " 2. You should add `1e-4*np.eye(...)` to the coefficient matrix to prevent singular value errors. Our test cases assume the coefficient `1e-4`, which is **not** equal to `np.exp(-4)`.\n", " 3. Do not include a bias term/constant column."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class LinearRegression():\n", "    \"\"\" Perform linear regression and predict the output on unseen examples. \n", "    \n", "    attributes: \n", "        beta (np.ndarray) : vector containing parameters for the features\n", "    \"\"\"\n", "\n", "    def train(self, X, y):\n", "        \"\"\" Train the linear regression model by computing the estimate of the parameters\n", "        You should store the model parameters in self.beta, overwriting parameters as necessary.\n", "\n", "        args: \n", "            X (np.ndarray[num_examples, num_columns]) : matrix of training data\n", "            y (np.ndarray[num_examples]) : vector of output variables\n", "\n", "        return: LinearRegression -- returns itself (for convenience)\n", "        \"\"\"\n", "        self.beta = np.zeros(X.shape[1])\n", "        pass # Fill this in.\n", "    \n", "        return self\n", "\n", "    def predict(self, X_p): \n", "        \"\"\" Use the learned model to predict the output of X_p\n", "\n", "        args: \n", "            X_p (np.ndarray[num_examples, num_columns]) matrix of test/validation data where each row corresponds to an example\n", "\n", "        return: \n", "            (np.ndarray[num_examples]) vector of predicted outputs\n", "        \"\"\"\n", "        pass # Fill this in."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def linear_regression_instance_test(linear_regression_instance):\n", "    lr = linear_regression_instance()\n", "\n", "    # If this throws a Singular Matrix error, you did not add the smoothing term:\n", "    # If you get a reference value of [10000.0, 10000.0, 10000.0, 10000.0, 10000.0], then you are applying\n", "    #   smoothing incorrectly. You should not be adding the smoothing term to X.\n", "    test.equal(lr.train(np.zeros((20, 5)), np.ones((20,))).beta.tolist(), [0.0]*5)\n", "\n", "    # Basic functionality tests:\n", "    test.equal(lr.train(np.eye(6)*(1-1e-4), np.ones((6,))).beta.round(4).tolist(), [1.0]*6)\n", "    test.equal(lr.train(np.array([[0., 1.], [1., 2.], [2., 3.]]), np.array([1., 2., 3.])).beta.round(4).tolist(), [0.0001, 0.9999])\n", "    \n", "# Don't remove this function; we use it for the auto-grader.\n", "@test\n", "def linear_regression_instance():\n", "    return LinearRegression()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Error Functions\n", "\n", "One last part to this: linear regression minimizes the mean-squared-error. Write a function that calculates the mean mean-squared-error when given a prediction and a ground-truth vector."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def mean_squared_error_test(mean_squared_error):\n", "    test.equal(mean_squared_error(np.ones(10), np.ones(10)), 0)\n", "    test.equal(mean_squared_error(np.ones(10), np.zeros(10)), 1)\n", "\n", "@test\n", "def mean_squared_error(pred, ground_truth):\n", "    \"\"\" calculate the mean mean-squared-error between pred and ground_truth\n", "    \n", "    args:\n", "      pred : np.ndarray[num_examples] -- the predictions\n", "      ground_truth : np.ndarray[num_examples] -- the ground truth values\n", "      \n", "    returns: float -- the average mean-squared-error between predictions and ground_truth values.\n", "    \"\"\"\n", "    pass"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Putting it all together\n", "\n", "And finally, lets run the entire pipeline end-to-end. You should put all the functions you have written so far together to:\n", "\n", "1. read and split the dataset,\n", "2. train two separate models on the training set, one to predict `MgrWant` and the other to predict `CareerSat`,\n", "3. perform inference on the validation set, and\n", "4. return the mean-squared error for each.\n", "\n", "Remember not to include both columns `MgrWant` and `CareerSat` when training models to predict either column. (i.e. when training `MgrWant`, you should not include `CareerSat` and vice-versa.)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def linear_regression_run_test(linear_regression_run):\n", "    mse_mgr, mse_sat = linear_regression_run(*read_csv())\n", "    test.true(np.abs(mse_mgr - 0.07214) < 1e-4)\n", "    test.true(np.abs(mse_sat - 1.29104) < 1e-4)\n", "\n", "@test\n", "def linear_regression_run(headers, rows):\n", "    \"\"\" Perform linear regression on (headers, rows), and return the MSE on the validation set for both `MgrWant` and `CareerSat`. \n", "\n", "    args: \n", "        headers : List[str] -- headers from CSV file\n", "        rows : np.ndarray[num_examples, num_columns] -- data from the CSV file\n", "        \n", "    return: Tuple[MSEMgrWant, MSECareerSat], where\n", "        MSEMgrWant : float -- the MSE between the predictions and the ground truth values for the column `MgrWant`.\n", "        MSECareerSat : float -- the MSE between the predictions and the ground truth values for the column `CareerSat`.\n", "    \"\"\"\n", "    return 0.0, 0.0"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Note that `MgrWant` is a binary variable, with range $[-1, 1]$, and `CareerSat` has range $[-2, 2]$; relative to the range, the mean-squared error for `CareerSat` is much smaller than `MgrWant`. This means that we can better predict `CareerSat` than `MgrWant`. Great!"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Your Turn\n", "\n", "Now that we've walked through this once with a large dataset, it is your turn to do this. You will be using [FiveThirtyEight's The Ultimate Halloween Candy Power Ranking](https://www.kaggle.com/fivethirtyeight/the-ultimate-halloween-candy-power-ranking/), included (and shuffled) as `candy.csv.gz`. (The dataset is Copyright (c) 2014 ESPN Internet Ventures. Our shuffled version is released under the MIT License.)\n", "\n", "From the original documentation, here is a description of the columns:\n", "\n", "| Column | Description | type |\n", "| --- |:--- |:--- |\n", "| **`winpercent`** | The overall win percentage according to 269,000 matchups. | float |\n", "| `competitorname` | The bar name | string (don't use this) |\n", "| `chocolate` | Does it contain chocolate? | boolean (`y`, `n`) |\n", "| `fruity` | Is it fruit flavored? | boolean (`y`, `n`) |\n", "| `caramel` | Is there caramel in the candy? | boolean (`y`, `n`) |\n", "| `peanutalmondy` | Does it contain peanuts, peanut butter or almonds? | boolean (`y`, `n`) |\n", "| `nougat` | Does it contain nougat? | boolean (`y`, `n`) |\n", "| `crispedricewafer` | Does it contain crisped rice, wafers, or a cookie component? | boolean (`y`, `n`) |\n", "| `hard` | Is it a hard candy? | boolean (`y`, `n`) |\n", "| `bar` | Is it a candy bar? | boolean (`y`, `n`) |\n", "| `pluribus` | Is it one of many candies in a bag or box? | boolean (`y`, `n`) |\n", "| `sugarpercent` | The percentile of sugar it falls under within the data set. | float |\n", "| `pricepercent` | The unit price percentile compared to the rest of the set. | float |\n", "\n", "\n", "You must predict `winpercent` using exactly **four** other columns. Use the first 20% of the dataset as the validation set (the dataset has already been shuffled for you). As output, you should provide the names of the columns and the validation of the MSE. Your MSE must be no more than `330`.\n", "\n", "You should convert boolean columns using `type_boolean`, and `LinearRegression` to perform the regression. Don't implement a bias term/constant column. Feel free to create new helper functions as necessary."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def candy_test(candy):\n", "    headers, mse = candy(*read_csv(\"candy.csv.gz\"))\n", "    #print((headers, mse))\n", "    test.true(len(headers) == 4)\n", "    test.true(\"winpercent\" not in headers)\n", "    test.true(mse < 330.)\n", "\n", "@test\n", "def candy(headers, data):\n", "    \"\"\" predict winpercent using no more than four other columns\n", "    \n", "    args:\n", "        headers : List[str] -- headers read from the csv file\n", "        data : List[List[str]] -- data from the csv file\n", "\n", "    returns: Tuple[selected_headers, mse]\n", "        selected_headers : List[str] -- the headers of at most four columns used to train the model\n", "        mse : float -- the mean-squared error when the columns in selected_headers are used to predict `winpercent`\n", "    \"\"\"\n", "    return [], 0.0"]}], "metadata": {"file_extension": ".py", "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.7"}, "mimetype": "text/x-python", "name": "python", "npconvert_exporter": "python", "pygments_lexer": "ipython3", "version": 3}, "nbformat": 4, "nbformat_minor": 2}