{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Time Series\n", "\n", "In this problem you will be analysing and visualizing time-series data. Specifically, you will be working with Pittsburgh Port Authority's TrueTime data which is [publicly available](http://truetime.portauthority.org/bustime/login.jsp). If you're interested, you can request an API key and collect the data yourself, but we've already collected a subset of the available data for the purposes of this assignment.\n", "\n", "We will be using [`pandas`](https://pandas.pydata.org/pandas-docs/stable/user_guide/) to work with this data."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "import pandas as pd\n", "import numpy as np\n", "import sqlite3\n", "import gmaps\n", "from testing.testing import test\n", "\n", "global gmaps_fig\n", "gmaps_fig = None"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## TrueTime dataset\n", "\n", "The bus data has been collected by querying the TrueTime API every minute. Each time, we make a request for vehicle information for every bus running on the 61A, 61B, 61C, and 61D bus routes. The results are given as XML, which are consequently parsed and stored within the `vehicles` table of a [sqlite](https://www.sqlite.org/index.html) database. (Ignore other tables in the database.)\n", "\n", "| | **vehicles** | \n", "|----------|:-------------|\n", "| vid      | vehicle identifier |\n", "| tmstmp | date and time of the last positional update of the vehicle |\n", "| lat | latitude position of the vehicle in decimal degrees |\n", "| lon | longitude position of the vehicle in decimal degrees |\n", "| hdg | heading of vehicle as a 360 degree value (0 is north, 90 is east, 180 is south, and 270 is west |\n", "| pid | pattern ID of trip currently being executed | \n", "| rt | route that is currently being execute | \n", "| des | destination of the current trip | \n", "| pdist | linear distance (feet) vehicle has traveled into the current pattern |\n", "|  spd | speed as reported from the vehicle in miles per hour | \n", "| tablockid | TA's version of the scheduled block identifier for work currently behind performed |\n", "| tatripid | TA's version of the scheduled trip identifier for the vehicle's current trip |\n", "\n", "First you will need to read in the data. We have dumped the raw form of the data into a sqlite database, which you can read directly into a pandas dataframe using `pd.read_sql_query`. You need to read the data in and do this:\n", "\n", "1. Sometimes the TrueTime API returns a bogus result that has empty strings for the `vid`. You should remove all rows that have blank `vid`s. \n", "2. SQLite does not enforce types on the data, so pandas reads most columns as objects even if the underlying type is an integer or float. To run numerical functions on them you need to convert numeric columns to the correct type.\n", "3. You need to set the timestamps as `pd.DatetimeIndex` and set them to be the Dataframe index. (You may need to wrap the type conversion like this `... = pd.DatetimeIndex(...)` to make it work.) Pandas will prepare the data for efficient time-based querying accordingly. You should not retain a `tmstmp` column.\n", "\n", "Note that strings show up as objects. This is because the underlying implementation of Pandas uses numpy arrays, which need fixed-size entries, so they store pointers to strings instead of the strings themselves.\n", "\n", "### Specification\n", "\n", "Your dataframe datatypes should match the datatypes in the test function below. There must be no columns where `vdf['vid']` is a blank string."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def load_data_test(load_data):\n", "    vdf = load_data()\n", "    \n", "    VDF_TYPES = {\n", "        'vid': \"int64\",\n", "        'lat': \"float64\",\n", "        'lon': \"float64\",\n", "        'hdg': \"int64\",\n", "        'pid': \"int64\",\n", "        'pdist': \"int64\",\n", "        'spd': \"int64\",\n", "        'tatripid': \"int64\",\n", "        'rt': 'object',\n", "        'des': 'object'\n", "    }\n", "\n", "    test.equal(VDF_TYPES, { k: str(vdf[k].dtypes) for k in VDF_TYPES })\n", "\n", "    # No entries with blank vehicle IDs:\n", "    # We use np.string_() instead of a str() because numpy will barf if the\n", "    # type of the RHS is different from the LHS.\n", "    test.equal(any(vdf['vid'].eq(np.string_())), False)\n", "\n", "    # Check number of entries:\n", "    test.equal(len(vdf), 215473)\n", "\n", "    # Set index to be a DatetimeIndex\n", "    test.true(isinstance(vdf.index, pd.DatetimeIndex))\n", "\n", "@test\n", "def load_data(fname='bus_aug23.db'):\n", "    \"\"\" Read the given database into a pandas dataframe.\n", "    \n", "    Args: \n", "        fname (string): filename of sqlite3 database to read\n", "        \n", "    Returns:\n", "        pd.DataFrame : a dataframe with the vehicle data \n", "    \"\"\"    \n", "    pass"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Splitting Data by Vehicle\n", "\n", "Now that the data is loaded, we will split the `vehicle` dataframe into dataframes for each individual vehicle. (Note that the same vehicle will run different routes at different times; this is expected behavior.)\n", "\n", "You use the [`groupby`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html) feature of pandas to (as the name suggests) group the dataframe by the `vid`  field. Convert the result into a list-of-dataframes.\n", "\n", "Note that `groupby` doesn't create a copy of the underlying data. Instead, it creates a new dataframe that points to some rows within the original data; this is vastly more memory-efficient especially when working with image or video data. We also include a test to make sure you didn't accidentally copy it.\n", "\n", "We strongly suggest you read the [`split-apply-combine` pattern](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html) guide; that is a powerful pattern that we will be using repeatedly in this assignment."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def split_vehicles_test(split_vehicles):\n", "    vdf = load_data()\n", "    vehicle_list = split_vehicles(vdf)\n", "\n", "    # Check that you have the correct number of vehicles:\n", "    test.equal(len(vehicle_list), len(vdf['vid'].unique()))\n", "    # Check type of entries:\n", "    test.true(all(isinstance(v, pd.DataFrame) for v in vehicle_list))\n", "    # This checks that the total number of entries is the same.\n", "    test.equal(sum(len(t) for t in vehicle_list), len(vdf))\n", "    # Make sure you've created a view of the data, not copied it:\n", "    test.true(all(v._is_copy for v in vehicle_list))\n", "\n", "@test\n", "def split_vehicles(df):\n", "    \"\"\" Splits the dataframe into a list of dataframes for each individual vehicle. \n", "    \n", "    Args: \n", "        df (pd.DataFrame): A dataframe containing all data\n", "        \n", "    Returns: \n", "        (list): A list of dataframes, where each dataframe contains vehicle data for a single vehicle\n", "    \"\"\"\n", "    pass\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Visualizing Speeds over Time\n", "\n", "Great! Now that we are all warmed up, lets try to extract some more information from this dataset. We want to see if traffic is slower at some times during the day than others.\n", "\n", "We can't directly measure the speed of traffic using the dataset, but we can infer this from the average speed of each bus (which we assume depends on the speed of traffic). We begin by writing a function that:\n", "\n", "1. selects all entries lying between `time_start` and `time_end` (inclusive) on any day within that,\n", "2. selects only data that falls on weekdays,\n", "3. groups entries by `vid`, and\n", "4. calculates the mean speed for each group\n", "\n", "The output should be a Series with the index `vid` (vehicle id) and the value `spd`, which is the mean recorded speed for each `vid` in the time range on any weekday in the dataset.\n", "\n", "**Hints**:\n", "\n", "1. Remember when we set the index to be a DatetimeIndex? This allows you to use [special lookup functions](https://pandas.pydata.org/pandas-docs/version/0.23.4/api.html#id7).\n", "2. We only want the data where `dayofweek < 5`.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def get_speeds_test(get_speeds):\n", "    vdf = load_data()\n", "    test.equal(get_speeds(vdf, \"10:00am\", \"10:30am\").index.names, ['vid'])\n", "    test.equal(get_speeds(vdf, \"10:00am\", \"10:30am\")[3200], 18.220338983050848)\n", "    test.equal(len(get_speeds(vdf, \"9:00am\", \"9:30am\")), 106)\n", "    test.equal(len(get_speeds(vdf, \"12:00am\", \"12:30am\")), 48)\n", "\n", "@test\n", "def get_speeds(df, time_start, time_end):\n", "    \"\"\" Calculate the mean speed by vehicles in a time range on weekdays.\n", "    \n", "    Args: \n", "        df (pd.DataFrame): A dataframe containing vehicle data\n", "        \n", "    Returns: \n", "        (pd.Series): with index `vid` and value `spd`, where:\n", "            `vid` -- is the vehicle id\n", "            `spd` -- the mean recorded speed in the time range on weekdays \n", "    \"\"\"\n", "    pass\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now that we have that function, lets visualize it as a heatmap! The x-axis represents time-of-day, from 00:00am to 11:59pm in steps of 15 minutes. The y-axis represents average speed in bins defined by `speed_bins`. The value of each cell corresponds to the number of buses reaching that speed in the time window.\n", "\n", "We've _almost_ completed this visualization for you. All you have to do is fill in the `set_heatmap_values` function to take a list-of-values and populate the heatmap matrix. The function [`numpy.histogram`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html) will help you with the heavy lifting for this function. Note that `plt.imshow` (which we use to display the image) has the y-axis increasing downwards. You must reverse the order of rows produced by `numpy.histogram` for the image to display correctly."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def set_heatmap_values_test(set_heatmap_values):\n", "    # Minimal input:\n", "    test.equal(set_heatmap_values([[0]], np.zeros((1, 1)), [0, 1]).tolist(), [[1.]])\n", "    # Check counting:\n", "    test.equal(set_heatmap_values([[0, 1]*5], np.zeros((2, 1)), [0, 1, 2]).tolist(), [[5.], [5.]])\n", "    # Make sure the order is reversed:\n", "    test.equal(set_heatmap_values([[0, 1, 1, 2, 2, 2, 3, 3, 3, 3, 4, 4]], np.zeros((5, 1)), [0, 1, 2, 3, 4, 5]).tolist(),\n", "              [[2.], [4.], [3.], [2.], [1.]])\n", "\n", "@test\n", "def set_heatmap_values(speed_data, hmap, speed_bins):\n", "    \"\"\"mutate each row of the heatmap with histogram data generated from the corresponding list in speed_data\n", "    \n", "    params:\n", "        speed_data : List[List[np.float64]] -- a list of lists that were returned by get_speeds;\n", "            each inner list corresponds to the speed values of all buses in that slice of time\n", "        hmap : np.ndarray[speeds, timeslice] -- a Numpy array where the cell at [i,j]\n", "            should be set to the number of vehicles traveling at a particular average speed and at a particular time.\n", "            In particular, hmap[:,i] should contain a histogram of the values in speed_data[i], but in reverse order.\n", "        speed_bins : List[int] -- interpret this argument like the argument `bins` in `numpy.histogram`.\n", "    \"\"\"\n", "    pass\n", "\n", "    return hmap # We don't need to return the value, but this makes it easier to test."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def visualize_speeds_test(visualize_speeds):\n", "    visualize_speeds(load_data())\n", "\n", "def prepare_speeds_figure(period):\n", "    fig = plt.figure(num=None, figsize=(8, 6), dpi=150)\n", "    plt.xlabel(\"Time of Weekday\")\n", "    ticklabels = zip(range(len(period)), [str(p).split()[-1] for p in period])\n", "    plt.xticks(*zip(*((p) for i, p in enumerate(ticklabels) if i % 8 == 0)))\n", "    plt.ylabel(\"Instantaneous Speed (mi/h)\")\n", "    return fig\n", "\n", "@test\n", "def visualize_speeds(vdf, interval=\"15T\"):\n", "    period = pd.period_range(start='12:00am', end='11:59pm', freq=interval)\n", "    speed_bins = list(range(0, 50, 2))\n", "\n", "    speed_data = [get_speeds(vdf, f\"{st.hour}:{st.minute}\", f\"{en.hour}:{en.minute}\").tolist()\n", "                  for st, en in zip(period[:-1], period[1:])]\n", "    hmap = np.zeros((len(speed_bins) - 1, len(period) - 1))\n", "\n", "    # You have to fill this function in above:\n", "    set_heatmap_values(speed_data, hmap, speed_bins)\n", "\n", "    fig = prepare_speeds_figure(period)\n", "    plt.imshow(hmap, cmap='hot', interpolation='nearest', extent=[0, len(period) - 1, 0, 50])\n", "    plt.colorbar()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can see a clear dip in speed from 15:00--18:00; that's the evening rush hour. The average speed improves as it gets later in the evening. Great!"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Visualizing Bus Bunching\n", "\n", "That's interesting, but not exactly useful. Let's apply this to a real-world problem: [Bus Bunching](https://en.wikipedia.org/wiki/Bus_bunching), where buses on similar routes tend to clump together, reducing the tail performance of the travel network. [Here's a real-world example from Singapore.](https://landtransportguru.net/bus-bunching/)\n", "\n", "Buses on the same route and direction have the same `pid` (pattern id) and their progress along the route is given by `pdist`. We'll do this by:\n", "\n", "1. grouping the data by `pid`, 10-minute blocks, and then by the bus (i.e. `vid`) \n", "2. calculating the average `pdist` value for each bus, and--\n", "3. calculating the successive difference of `pdist` values\n", "4. dropping `NaN` values\n", "\n", "We'll begin by doing the first three steps.\n", "\n", "Here are some tips and potential pitfalls:\n", "\n", "- Perform the grouping first by `pid`, then by time, and then by the bus `vid`. This can be done in a single call to `groupby`.\n", "- Look at `pandas.Grouper` to help you group by time.\n", "- Look for documentation for `reset_index()`, `sort_values()`, `groupby()`, and `groupby(...).diff()` for the rolling difference. I suggest avoiding `groupby(level=...).diff()`; instead note that running `groupby([index1, index2])` on a `pandas.Series` already grouped with `groupby([index1, index2])` takes minimal additional computation.\n", "- To help you check your progress: once the rolling difference is calculated and you have removed `NaN`s, you should have 16,462 records.\n", "- This is a difficult problem, persist in solving it."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def mean_pdist_test(mean_pdist):\n", "    pdists_rd = mean_pdist(load_data())\n", "    test.true(isinstance(pdists_rd, pd.core.series.Series))\n", "    \n", "    # Logarithmically spaced bins:\n", "    logbins = np.logspace(np.log10(1e2),np.log10(10e5),16)\n", "    binvals, _ = np.histogram(pdists_rd, bins=logbins)\n", "    test.true(np.array_equal(binvals, np.array([12, 41, 56, 88, 197, 390, 976, 3305, 6367, 4550, 455, 0, 0, 0, 0])))\n", "\n", "@test\n", "def mean_pdist(vdf):\n", "    \"\"\"calculate the mean pdist value \n", "    \n", "    params:\n", "        vdf : pd.Dataframe -- the loaded dataframe\n", "        \n", "    returns: pd.Series with indices:\n", "        - `pid`   : the pattern id\n", "        - `tmstmp`: the time, grouped to 10-minute blocks\n", "        and value equal to the rolling difference between \n", "    \"\"\"\n", "\n", "    pass"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now that that works, lets visualize this!"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def mean_pdist_histogram_test(mean_pdist_histogram):\n", "    mean_pdist_histogram(mean_pdist(load_data()).tolist())\n", "\n", "@test\n", "def mean_pdist_histogram(x):\n", "    \"\"\"Plot a histogram with logarithmic bins.\n", "\n", "    args:\n", "      x : List[float] -- the list of inter-bus distances\n", "\n", "    returns: The result of calling plt.hist to draw the histogram\n", "    \"\"\"\n", "\n", "    logbins = np.logspace(np.log10(1e2),np.log10(10e5),16)\n", "    \n", "    rv = [] # ...fill this line in\n", "    assert False # Remove this line when done\n", "    \n", "    plt.plot(\n", "        (logbins[:-1] + logbins[1:])/2,\n", "        [  12.,   41.,   56.,   88.,  197.,  390.,  976., 3305., 6367., 4550.,  455.,    0.,    0.,    0.,    0.])\n", "    plt.xscale('log')\n", "    plt.show()\n", "    return rv"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We see that the buses are well-behaved most of the time, but they occasionally move too close together. We arbitrarily define a quarter of a mile (1320 ft) as the cutoff for counting bus bunching. Lets drill down on this and see _where_ in Pittsburgh this happens. We'll use a handy little library called [`gmaps`](https://github.com/pbugnion/gmaps) to handle the visualization.\n", "\n", "Here's the final part of this homework: given the `pid` and `tmstmp` as grouped in `mean_pdist`, retrieve the `lat` and `lon` of _any_ bus in the original dataset. (This is a simplification to make your homework more manageable.)\n", "\n", "We will do this using `pd.merge_asof` which, when given a `left` table, can be used to find and merge the closest row in the `right` table (where closest means numerically, by time, etc.) For efficient merging, it expects the indexes/columns you are merging by to be sorted. The desired output is a list of `Tuple[lat, lon]` values.\n", "\n", "Here are some tips:\n", "\n", "- use `pandas.merge_asof`. Make sure you pass `by=\"pid\"` so it matches the closest `tmstmp` between the `left` and `right` rows with the same `pid`.\n", "- the search direction should be 'backward', which is the default value\n", "- merge a `left` column with a `right` index\n", "  - the right index is already the `tmstmp`; you just need to sort the index\n", "  - the left `Series` needs to have its indexes reset and then the `tmstmp` column sorted.\n", "- I've included a Google Maps JavaScript API key that only works if your Jupyter Notebook is displayed at `http://localhost:8888/notebooks/time_series.ipynb#`; you may need to create your own from https://console.developers.google.com/apis using a non-Andrew email.\n", "- You need to run the command `jupyter nbextension enable --py gmaps` to enable the `gmaps` javascript extension. Do that and then restart the Jupyter Notebook server."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["gmaps_fig = None\n", "\n", "def bunched_bus_location_test(bunched_bus_location):\n", "    data = load_data()\n", "    left = mean_pdist(data)\n", "    left = left[left<1320] # Only keep instances where the gap between buses is 1/4mi\n", "    bunches = bunched_bus_location(left, data)\n", "    \n", "    test.equal(len(bunches), 259)\n", "\n", "    if len(bunches) != 259:\n", "        return;\n", " \n", "    # You may need to change this API Key\n", "    # This will only work if your addressbar reads `http://localhost:8888/notebooks/time_series.ipynb#`\n", "    # You need to run the command `jupyter nbextension enable --py gmaps` to enable\n", "    # the `gmaps` javascript extension. Do that and then restart the Jupyter Notebook server.\n", "    gmaps.configure(api_key=\"AIzaSyAoCoKcM-skvPYRI-w5KvMTr6FYRiXHhk8\")\n", "    global gmaps_fig\n", "    gmaps_fig = gmaps.figure()\n", "    gmaps_fig.add_layer(gmaps.heatmap_layer(bunches))\n", "\n", "@test\n", "def bunched_bus_location(left, right):\n", "    \"\"\"Get locations for the bunched buses as described above\n", "    \n", "    args:\n", "      left : pd.Series -- the output of mean_pdist\n", "      right: pd.DataFrame -- the original DataFrame\n", "      \n", "    returns : List[Tuple[float, float]] -- a list of (lat, lon) pairs from `right` that match `left`. \n", "    \"\"\"\n", "    pass\n", "\n", "gmaps_fig # This is here to make Jupyter produce the map."]}], "metadata": {"anaconda-cloud": {}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.4"}}, "nbformat": 4, "nbformat_minor": 1}